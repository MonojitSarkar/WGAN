{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport string\nimport numpy as np\nimport random\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom IPython.core.debugger import set_trace","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-30T15:35:25.501090Z","iopub.execute_input":"2024-03-30T15:35:25.501645Z","iopub.status.idle":"2024-03-30T15:35:27.482860Z","shell.execute_reply.started":"2024-03-30T15:35:25.501612Z","shell.execute_reply":"2024-03-30T15:35:27.481539Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_alphabets_random(window, shift, text=None, reverse=False):\n    if text is not None:\n        shift = shift % len(text)\n        shifted_text = text[shift:] + text[:shift]\n        return f'Input: {text}\\nShifted: {shifted_text}'\n    \n    alphabets = string.ascii_lowercase\n    rand_indices = [random.randint(0, 25) for _ in range(window)]\n    \n    text = ''.join(list(map(lambda x: alphabets[x], rand_indices)))\n    \n    if reverse:\n        reversed_text = ''.join(list(reversed(list(text))))\n        \n    shift = shift % len(text)\n    shifted_text = text[shift:] + text[:shift]\n    \n    return text, shifted_text\n\nget_alphabets_random(10, -2, reverse=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:27.484896Z","iopub.execute_input":"2024-03-30T15:35:27.485371Z","iopub.status.idle":"2024-03-30T15:35:27.499342Z","shell.execute_reply.started":"2024-03-30T15:35:27.485339Z","shell.execute_reply":"2024-03-30T15:35:27.498016Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"('zpoyqqytua', 'uazpoyqqyt')"},"metadata":{}}]},{"cell_type":"code","source":"def generate_training_data(window, shift, num_examples):\n    x = list(); y = list()\n    \n    for _ in range(num_examples):\n        text, shifted = get_alphabets_random(window, shift, reverse=False)\n        x.append(text); y.append(shifted)\n        \n    random_id = random.randint(0, len(x))\n    print(x[random_id], y[random_id])\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:27.500958Z","iopub.execute_input":"2024-03-30T15:35:27.501289Z","iopub.status.idle":"2024-03-30T15:35:27.510893Z","shell.execute_reply.started":"2024-03-30T15:35:27.501260Z","shell.execute_reply":"2024-03-30T15:35:27.509760Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def create_vocabulary():\n    unique_characters = list(string.ascii_lowercase)\n    char_to_id = {char:i for i, char in enumerate(unique_characters)}\n    id_to_char = {i: char for i, char in enumerate(unique_characters)}\n    \n    features_dim = len(unique_characters)\n    print(f'Number of features is {features_dim}')\n    return char_to_id, id_to_char, features_dim","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:27.514945Z","iopub.execute_input":"2024-03-30T15:35:27.516106Z","iopub.status.idle":"2024-03-30T15:35:27.523734Z","shell.execute_reply.started":"2024-03-30T15:35:27.516059Z","shell.execute_reply":"2024-03-30T15:35:27.522413Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"encode_text = lambda x: [char_to_id[_] for _ in x]\ndecode_text = lambda x_: [id_to_char[_] for _ in x_]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:27.525323Z","iopub.execute_input":"2024-03-30T15:35:27.525970Z","iopub.status.idle":"2024-03-30T15:35:27.533721Z","shell.execute_reply.started":"2024-03-30T15:35:27.525929Z","shell.execute_reply":"2024-03-30T15:35:27.532406Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def encode_data(x, y, features_dim):\n    encoded_x = list(map(lambda x_: encode_text(x_), x))\n    encoded_y = list(map(lambda x_: encode_text(x_), y))\n    \n    one_hot_x = F.one_hot(torch.tensor(encoded_x), num_classes=features_dim)\n    one_hot_y = F.one_hot(torch.tensor(encoded_y), num_classes=features_dim)\n    \n    one_hot_y = one_hot_y.permute(1, 0, 2)\n    \n    return one_hot_x.to(torch.float32), one_hot_y.to(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:27.534799Z","iopub.execute_input":"2024-03-30T15:35:27.535767Z","iopub.status.idle":"2024-03-30T15:35:27.545962Z","shell.execute_reply.started":"2024-03-30T15:35:27.535736Z","shell.execute_reply":"2024-03-30T15:35:27.544611Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"x, y = generate_training_data(10, -2, 10000)\nchar_to_id, id_to_char, features = create_vocabulary()\none_hot_x, one_hot_y = encode_data(x, y, features)\nprint(one_hot_x.dtype, one_hot_y.dtype)\n\nval_x, val_y = generate_training_data(10, -2, 1000)\nval_x, val_y = encode_data(val_x, val_y, features)\nprint(val_x.shape, val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:27.547466Z","iopub.execute_input":"2024-03-30T15:35:27.548702Z","iopub.status.idle":"2024-03-30T15:35:28.087766Z","shell.execute_reply.started":"2024-03-30T15:35:27.548667Z","shell.execute_reply":"2024-03-30T15:35:28.086489Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"bqiexttorv rvbqiextto\nNumber of features is 26\ntorch.float32 torch.float32\nouuzfrrunh nhouuzfrru\ntorch.Size([1000, 10, 26]) torch.Size([10, 1000, 26])\n","output_type":"stream"}]},{"cell_type":"code","source":"decode_text(torch.argmax(one_hot_x[0], dim=-1).tolist()), decode_text(torch.argmax(one_hot_y[:, 0, :], dim=-1).tolist())","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:28.091631Z","iopub.execute_input":"2024-03-30T15:35:28.092557Z","iopub.status.idle":"2024-03-30T15:35:28.102678Z","shell.execute_reply.started":"2024-03-30T15:35:28.092518Z","shell.execute_reply":"2024-03-30T15:35:28.101287Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(['h', 'u', 'l', 'z', 'f', 'o', 'e', 'r', 'h', 'e'],\n ['h', 'e', 'h', 'u', 'l', 'z', 'f', 'o', 'e', 'r'])"},"metadata":{}}]},{"cell_type":"code","source":"class CustomData(Dataset):\n    def __init__(self, x_data, y_data):\n        super().__init__()\n        self.x_data = x_data\n        self.y_data = y_data\n        \n    def __len__(self):\n        return len(self.x_data)\n    \n    def __getitem__(self, idx):\n        return self.x_data[idx], self.y_data[:, idx, :]\n    \ndataset = CustomData(one_hot_x, one_hot_y)\nval_dataset = CustomData(val_x, val_y)\n\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:35:28.103980Z","iopub.execute_input":"2024-03-30T15:35:28.104307Z","iopub.status.idle":"2024-03-30T15:35:28.113459Z","shell.execute_reply.started":"2024-03-30T15:35:28.104279Z","shell.execute_reply":"2024-03-30T15:35:28.112463Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class AttentionModel(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training):\n        super().__init__()\n        self.timesteps = timesteps\n        self.input_features_dim = input_features_dim\n        \n        self.densor1 = nn.Linear(decoder_dim+(2*encoder_dim), 10)\n        self.tanh = nn.Tanh()\n        self.densor2 = nn.Linear(10, 1)\n        self.softmax = nn.Softmax(dim=1)\n        \n        self.pre_attention_lstm = nn.LSTM(input_size=input_features_dim, hidden_size=encoder_dim, batch_first=True, bidirectional=True)\n        self.post_attention_lstm = nn.LSTM(input_size=encoder_dim*2, hidden_size=decoder_dim, batch_first=True)\n        self.output_layer = nn.Linear(decoder_dim, output_features_dim)\n        \n        \n    def forward(self, x, s0, c0):\n        # x-> (B, timesteps, features) -> (128, 10, 26)\n        # s0, c0 -> (1, B, decoder_dim) -> (1, 128, 16)\n        #set_trace()\n        self.s = s0; self.c = c0\n        encoder_hidden_states, (hn, cn) = self.pre_attention_lstm(x) #hidden_states: (B, timesteps, encoder_dim*2) -> (128, 10, 16)\n        outputs = list()\n        \n        for t in range(self.timesteps):\n            context = self.one_step_attention(encoder_hidden_states) # (B, timesteps, 2*encoder_dim)\n            _, (self.s, self.c )= self.post_attention_lstm(context, (self.s, self.c)) # _ -> (B, timesteps, decoder_dim)\n            output = self.output_layer(self.s) # (1, B, output_features_dim) -> (1, 128, 26)\n            outputs.append(output) # (timesteps, B, output_features_dim) -> (10, 128, 26)\n        \n        return outputs\n    \n    def one_step_attention(self, encoder_hidden_states):\n        # encoder_Hidden_states -> (B, timesteps, 2*encoder_dim) -> (128, 10, 16)\n        # self.s -> (1, B, decoder_dim)\n        # 1. first the hidden state for decoder must be repeated to match the hidden states of encoder\n        # self.s -> (timesteps, B, decoder_dim) -> permute -> (B, timesteps, decoder_dim)\n        # 2. then concatenate the hidden state for decoder and the hidden state for encoder -> (B, timesteps, 2*encoder_dim + decoder_dim)\n        # pass it through the first dense layer\n        # pass it through the second dense layer\n        # use softmax to decide which hidden state of encoder is the most important\n        # use dot product to find the important hidden state of encoder and feed it as input to the decoder\n        hidden_decoder = self.s.repeat(10, 1, 1).permute(1, 0, 2) # (B, timesteps, decoder_dim) -> (128, 10, 16)\n        concat = torch.concatenate([encoder_hidden_states, hidden_decoder], dim=-1) # (B, timesteps, 2*encoder_dim + decoder_dim) -> (128, 10, 32)\n        e = self.tanh(self.densor1(concat)) # (B, timesteps, 10) -> (128, 10, 10)\n        energies = self.softmax(self.densor2(e)) # (B, timesteps, 1) -> (128, 10, 1)\n        # let's if without permute in next step if the code converges\n        energies = energies.repeat(1, 1, 10).permute(0, 2, 1) # (B, timesteps, 10) -> (B, 10, 10)\n        context = torch.bmm(energies, encoder_hidden_states) # (B, timesteps, timesteps) @ (B, timesteps, 2*encoder_dim) -> (B, timesteps, 2*encoder_dim)\n        \n        return context\n        \n    def predict(self, x):\n        assert len(x) == self.timesteps\n        encoded = encode_text(x)\n        one_hot = F.one_hot(torch.tensor(encoded), num_classes=self.input_features_dim)\n        s0 = torch.zeros(1, 1, decoder_dim); c0 = torch.zeros(1, 1, decoder_dim)\n        one_hot = one_hot.unsqueeze(0)\n        pred = self.forward(one_hot.to(torch.float32), s0, c0)\n        pred = ''.join(decode_text([torch.argmax(t, dim=-1).tolist()[0][0] for t in pred]))\n        return pred","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:41:42.113871Z","iopub.execute_input":"2024-03-30T15:41:42.114582Z","iopub.status.idle":"2024-03-30T15:41:42.135006Z","shell.execute_reply.started":"2024-03-30T15:41:42.114542Z","shell.execute_reply":"2024-03-30T15:41:42.133318Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(y_true, y_pred):\n    ce = nn.CrossEntropyLoss()\n    total = 0\n    for target, logit in zip(list(y_true), list(y_pred)):\n        loss = ce(logit[0], target)\n        total += loss\n        \n    return total","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:41:43.174453Z","iopub.execute_input":"2024-03-30T15:41:43.174906Z","iopub.status.idle":"2024-03-30T15:41:43.181544Z","shell.execute_reply.started":"2024-03-30T15:41:43.174869Z","shell.execute_reply":"2024-03-30T15:41:43.180282Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"encoder_dim = 8\ndecoder_dim = 16\ninput_features_dim = features\noutput_features_dim = features\ntimesteps = 10\nnum_training = 10000\nepochs = 40\n\ns0, c0 = torch.zeros(1, num_training, decoder_dim), torch.zeros(1, num_training, decoder_dim)\n\nam = AttentionModel(encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training)\nopt = optim.Adam(am.parameters())\n\nfor epoch in range(epochs):\n    am.train()\n    train_loss = list()\n    for i, (inputs, target) in enumerate(train_loader):\n        #set_trace()\n        target = target.permute(1, 0, 2)\n        batch_size = inputs.size(0)\n        s0 = torch.zeros(1, batch_size, decoder_dim); c0 = torch.zeros(1, batch_size, decoder_dim)\n        outputs = am(inputs, s0, c0)\n        \n        opt.zero_grad()\n        \n        total_loss = calculate_loss(target, outputs)\n        train_loss.append(total_loss)\n        \n        total_loss.backward()\n        opt.step()\n        \n    print(f'Epoch {epoch}:: Train Loss {torch.mean(torch.tensor(train_loss))}')\n    \n    am.eval()\n    with torch.no_grad():\n        s0 = torch.zeros(1, 1000, decoder_dim); c0 = torch.zeros(1, 1000, decoder_dim)\n        out = am(val_x, s0, c0)\n        \n        val_loss = calculate_loss(val_y, out)\n        \n        print(f'Epoch {epoch}:: Val Loss {val_loss}')\n        test = 'monojitabc'\n        pred = am.predict(test)\n        print(f'Input {test} --> Output {pred}')\n        print()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:41:56.505106Z","iopub.execute_input":"2024-03-30T15:41:56.505572Z","iopub.status.idle":"2024-03-30T15:44:07.798458Z","shell.execute_reply.started":"2024-03-30T15:41:56.505531Z","shell.execute_reply":"2024-03-30T15:44:07.797056Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 0:: Train Loss 32.61753845214844\nEpoch 0:: Val Loss 32.46696090698242\nInput monojitabc --> Output iiiiiiiiii\n\nEpoch 1:: Train Loss 31.962995529174805\nEpoch 1:: Val Loss 31.577430725097656\nInput monojitabc --> Output oooooooooo\n\nEpoch 2:: Train Loss 31.417795181274414\nEpoch 2:: Val Loss 31.297893524169922\nInput monojitabc --> Output oooooooooo\n\nEpoch 3:: Train Loss 31.16803741455078\nEpoch 3:: Val Loss 31.033721923828125\nInput monojitabc --> Output oooooooooo\n\nEpoch 4:: Train Loss 30.827407836914062\nEpoch 4:: Val Loss 30.787288665771484\nInput monojitabc --> Output oooooooooo\n\nEpoch 5:: Train Loss 30.51252555847168\nEpoch 5:: Val Loss 30.369169235229492\nInput monojitabc --> Output oooooooooo\n\nEpoch 6:: Train Loss 30.182771682739258\nEpoch 6:: Val Loss 30.01956558227539\nInput monojitabc --> Output oooooooooo\n\nEpoch 7:: Train Loss 29.752531051635742\nEpoch 7:: Val Loss 29.481632232666016\nInput monojitabc --> Output cooooooooo\n\nEpoch 8:: Train Loss 29.17110824584961\nEpoch 8:: Val Loss 28.879064559936523\nInput monojitabc --> Output tooooooooo\n\nEpoch 9:: Train Loss 28.54265022277832\nEpoch 9:: Val Loss 28.15410614013672\nInput monojitabc --> Output tcoooooooo\n\nEpoch 10:: Train Loss 27.599702835083008\nEpoch 10:: Val Loss 26.963930130004883\nInput monojitabc --> Output bcoooooooo\n\nEpoch 11:: Train Loss 26.279951095581055\nEpoch 11:: Val Loss 25.520038604736328\nInput monojitabc --> Output bcoooooooo\n\nEpoch 12:: Train Loss 24.238033294677734\nEpoch 12:: Val Loss 22.883970260620117\nInput monojitabc --> Output ccmoootttt\n\nEpoch 13:: Train Loss 21.05274772644043\nEpoch 13:: Val Loss 19.15789222717285\nInput monojitabc --> Output ccmoonemmt\n\nEpoch 14:: Train Loss 17.117605209350586\nEpoch 14:: Val Loss 15.448507308959961\nInput monojitabc --> Output ccmoooojiw\n\nEpoch 15:: Train Loss 12.845405578613281\nEpoch 15:: Val Loss 10.71540355682373\nInput monojitabc --> Output ccmonojitt\n\nEpoch 16:: Train Loss 8.897377967834473\nEpoch 16:: Val Loss 7.23127555847168\nInput monojitabc --> Output ccmonojita\n\nEpoch 17:: Train Loss 6.295590400695801\nEpoch 17:: Val Loss 5.308647632598877\nInput monojitabc --> Output bcmonojita\n\nEpoch 18:: Train Loss 4.669661045074463\nEpoch 18:: Val Loss 4.050225257873535\nInput monojitabc --> Output bcmonojita\n\nEpoch 19:: Train Loss 3.62994384765625\nEpoch 19:: Val Loss 3.1964337825775146\nInput monojitabc --> Output bcmonojita\n\nEpoch 20:: Train Loss 2.885152578353882\nEpoch 20:: Val Loss 2.5422613620758057\nInput monojitabc --> Output bcmonojita\n\nEpoch 21:: Train Loss 2.174781560897827\nEpoch 21:: Val Loss 1.842276692390442\nInput monojitabc --> Output bcmonojita\n\nEpoch 22:: Train Loss 1.6296645402908325\nEpoch 22:: Val Loss 1.4239170551300049\nInput monojitabc --> Output bcmonojita\n\nEpoch 23:: Train Loss 1.216829538345337\nEpoch 23:: Val Loss 1.275041103363037\nInput monojitabc --> Output bcmonojita\n\nEpoch 24:: Train Loss 0.9988383054733276\nEpoch 24:: Val Loss 0.8508273959159851\nInput monojitabc --> Output bcmonojita\n\nEpoch 25:: Train Loss 0.7841141819953918\nEpoch 25:: Val Loss 0.761976420879364\nInput monojitabc --> Output bcmonojita\n\nEpoch 26:: Train Loss 0.6562591791152954\nEpoch 26:: Val Loss 0.9532293081283569\nInput monojitabc --> Output bcmonojita\n\nEpoch 27:: Train Loss 0.5752258896827698\nEpoch 27:: Val Loss 0.5032224059104919\nInput monojitabc --> Output bcmonojita\n\nEpoch 28:: Train Loss 0.47725847363471985\nEpoch 28:: Val Loss 0.4374755620956421\nInput monojitabc --> Output bcmonojita\n\nEpoch 29:: Train Loss 0.41113021969795227\nEpoch 29:: Val Loss 0.416628360748291\nInput monojitabc --> Output bcmonojita\n\nEpoch 30:: Train Loss 0.36919039487838745\nEpoch 30:: Val Loss 0.39875662326812744\nInput monojitabc --> Output bcmonojita\n\nEpoch 31:: Train Loss 3.056321620941162\nEpoch 31:: Val Loss 0.5863260626792908\nInput monojitabc --> Output bcmonojita\n\nEpoch 32:: Train Loss 0.45124077796936035\nEpoch 32:: Val Loss 0.38336142897605896\nInput monojitabc --> Output bcmonojita\n\nEpoch 33:: Train Loss 0.3480386435985565\nEpoch 33:: Val Loss 0.3265811800956726\nInput monojitabc --> Output bcmonojita\n\nEpoch 34:: Train Loss 0.30142417550086975\nEpoch 34:: Val Loss 0.2908300459384918\nInput monojitabc --> Output bcmonojita\n\nEpoch 35:: Train Loss 0.26991716027259827\nEpoch 35:: Val Loss 0.2595612108707428\nInput monojitabc --> Output bcmonojita\n\nEpoch 36:: Train Loss 0.24429382383823395\nEpoch 36:: Val Loss 0.23536516726016998\nInput monojitabc --> Output bcmonojita\n\nEpoch 37:: Train Loss 0.2226283848285675\nEpoch 37:: Val Loss 0.21556445956230164\nInput monojitabc --> Output bcmonojita\n\nEpoch 38:: Train Loss 0.2048458606004715\nEpoch 38:: Val Loss 0.20340251922607422\nInput monojitabc --> Output bcmonojita\n\nEpoch 39:: Train Loss 0.18948368728160858\nEpoch 39:: Val Loss 0.18623597919940948\nInput monojitabc --> Output bcmonojita\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test = 'thisabcdifa'\npred = am.predict(test)\npred","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:44:26.694660Z","iopub.execute_input":"2024-03-30T15:44:26.695088Z","iopub.status.idle":"2024-03-30T15:44:26.772559Z","shell.execute_reply.started":"2024-03-30T15:44:26.695052Z","shell.execute_reply":"2024-03-30T15:44:26.770873Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthisabcdifa\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pred\n","Cell \u001b[0;32mIn[20], line 54\u001b[0m, in \u001b[0;36mAttentionModel.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps\n\u001b[1;32m     55\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m encode_text(x)\n\u001b[1;32m     56\u001b[0m     one_hot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39mtensor(encoded), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_features_dim)\n","\u001b[0;31mAssertionError\u001b[0m: "],"ename":"AssertionError","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"test_x, test_y = encode_data(x, y, features)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:38:11.104715Z","iopub.execute_input":"2024-03-30T15:38:11.105196Z","iopub.status.idle":"2024-03-30T15:38:11.262038Z","shell.execute_reply.started":"2024-03-30T15:38:11.105152Z","shell.execute_reply":"2024-03-30T15:38:11.260682Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"calculate_loss(one_hot_y, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:38:11.265040Z","iopub.execute_input":"2024-03-30T15:38:11.265468Z","iopub.status.idle":"2024-03-30T15:38:12.655207Z","shell.execute_reply.started":"2024-03-30T15:38:11.265429Z","shell.execute_reply":"2024-03-30T15:38:12.653090Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target, logit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mlist\u001b[39m(y_true), \u001b[38;5;28mlist\u001b[39m(y_pred)):\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (16) to match target batch_size (10000)."],"ename":"ValueError","evalue":"Expected input batch_size (16) to match target batch_size (10000).","output_type":"error"}]},{"cell_type":"code","source":"one_hot_y[:, 10, :]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:38:12.656337Z","iopub.status.idle":"2024-03-30T15:38:12.656801Z","shell.execute_reply.started":"2024-03-30T15:38:12.656577Z","shell.execute_reply":"2024-03-30T15:38:12.656596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.argmax(one_hot_y[0], dim=-1).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:38:12.658117Z","iopub.status.idle":"2024-03-30T15:38:12.658719Z","shell.execute_reply.started":"2024-03-30T15:38:12.658433Z","shell.execute_reply":"2024-03-30T15:38:12.658457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ce = nn.CrossEntropyLoss()\ntarget_label = torch.argmax(one_hot_y[0], dim=-1)\nprint(outputs[0].shape, target_label.shape)\nce(outputs[0][0], target_label), ce(outputs[0][0], one_hot_y[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:38:12.660132Z","iopub.status.idle":"2024-03-30T15:38:12.660705Z","shell.execute_reply.started":"2024-03-30T15:38:12.660414Z","shell.execute_reply":"2024-03-30T15:38:12.660437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out2[0].repeat(1, 10).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:38:12.662276Z","iopub.status.idle":"2024-03-30T15:38:12.663261Z","shell.execute_reply.started":"2024-03-30T15:38:12.663049Z","shell.execute_reply":"2024-03-30T15:38:12.663069Z"},"trusted":true},"execution_count":null,"outputs":[]}]}